{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2107b910",
   "metadata": {},
   "source": [
    "# Random Word Generator\n",
    "This code is a generator designed to create random words. It operates by training on a list of words or sentences supplied by the user.\n",
    "\n",
    "## Selecting your text file\n",
    "\n",
    "To pick the word list file, you can either open a file selection screen using tkinter, or write the relative path to your text file into the TXT variable below.\n",
    "\n",
    "### Using tkinter\n",
    "\n",
    "For selecting your text file with a file selection screen, please import and activate the tkinter module using the script below. If this fails, make sure you have pip installed and your python interpreter can access it. Run\n",
    "```console\n",
    "pip install tkinter\n",
    "```\n",
    "in your console to install tkinter to your system if it is not already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3048b9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725d48c",
   "metadata": {},
   "source": [
    "Now execute the code below to open the file selection screen whenever you want to change the wordlist file. If it does not seem to open, try looking for it in the alt+tab view, or minimize all windows until you can see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237496fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT = filedialog.askopenfilename(initialdir=\".\", filetypes=[(\"Text Files\", \"*.txt\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa7f14",
   "metadata": {},
   "source": [
    "### Defining the file manually\n",
    "Alternatively, you can enter the path to your wordlist into the variable like here. I have commented out alternative files I use for testing. These files are not actually included in the GitHub repository, as I recommend choosing your own. Then change the string to be the relative path to your file and execute the code snippet to save the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c53855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TXT = \"1-1000.txt\"\n",
    "# TXT = \"1000hÃ¤ufigste.txt\"\n",
    "# TXT = \"google-10000-english.txt\"\n",
    "TXT = \"wortliste.txt\"\n",
    "# TXT = \"testing.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfb5b3",
   "metadata": {},
   "source": [
    "## Training with the wordlist\n",
    "### Reading the wordlist\n",
    "Use the following code to initialize the wordlist or to tell the code to read it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1476ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TXT, 'r', encoding='utf-8') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ad5fa",
   "metadata": {},
   "source": [
    "### Generate the prediction cache\n",
    "#### Dynamic Token Prediction Encoder\n",
    "This version of the encoder counts every instance of every possible length of string (within a group). Generating words with the cache generated by this will mostly result in large parts of or entirely preexisting words, which you might consider as a lower quality result. **WARNING:** If your file is large, this code will generate even larger cache files. For example: 1000 words of english (seperated by newlines) compile to max 33000 lines using the dynamic encoder. That's half a megabyte.\n",
    "In the case of 10000 words, almost 550k lines will be written (10 MB).\n",
    "A full wordlist of almost 300k words will result in files around 50 million lines (1 GB).\n",
    "\n",
    "I recommend skipping this and using the n-length to one character encoder [here](#one-character-prediction-encoder-with-n-character-context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6001c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic token prediction encoder\n",
    "\n",
    "groups = txt.split('\\n')\n",
    "\n",
    "count = {}\n",
    "for group in groups:\n",
    "    for first_token_start in range(0, len(group) + 1):\n",
    "        for first_token_end in range(first_token_start, len(group) + 1):\n",
    "            next_token = None\n",
    "            first_token = \"\" if first_token_end == 0 else group[first_token_start:first_token_end]\n",
    "            for next_token_size in range(1, len(group) + 2):\n",
    "                if next_token_size == len(group) + 1 and first_token_end == 0:\n",
    "                    continue\n",
    "                new_next_token = group[first_token_end:first_token_end + next_token_size]\n",
    "                if next_token is not None and new_next_token == next_token:\n",
    "                    continue\n",
    "                next_token = \"\" if next_token_size == len(group) + 1 else new_next_token\n",
    "\n",
    "                if first_token not in count:\n",
    "                    count[first_token] = {}\n",
    "\n",
    "                if next_token not in count[first_token]:\n",
    "                    count[first_token][next_token] = 0\n",
    "\n",
    "                count[first_token][next_token] += 1\n",
    "\n",
    "import json, os\n",
    "if not os.path.exists('count.json'):\n",
    "    os.makedirs('count.json')\n",
    "with open('count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(count, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad4891",
   "metadata": {},
   "source": [
    "### One character prediction encoder with n character context\n",
    "This encoder is more disk efficient and produces more unfamiliar results by only generating one character at a time, but using the last n characters to determine the probabilities of each.\n",
    "With this encoder, even large files with 300k words will result in only a 2MB cache (100k lines).\n",
    "Execute the code below to compile our generator, if you have not used the inefficient one above already.\n",
    "If you want, you may change the value of n. This is the maximum amount of characters the encoder will enable to be used as context for the next token, default 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8457a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 to n letter to single letter prediction encoder\n",
    "\n",
    "n = 3\n",
    "\n",
    "groups = txt.split(\"\\n\")\n",
    "\n",
    "count = {}\n",
    "for group in groups:\n",
    "    for j in range(1, n + 1):\n",
    "        previous_next_letter = None\n",
    "        for i in range(j - 1, len(group) + 1):\n",
    "            firstTokenStart = max(0, i-j)\n",
    "            first_letter = group[firstTokenStart:i] if i > 0 else \"\"\n",
    "            \n",
    "            next_letter = \"\" if i == len(group) else group[i]\n",
    "            \n",
    "            if previous_next_letter is not None and next_letter == previous_next_letter:\n",
    "                continue\n",
    "            previous_next_letter = next_letter\n",
    "\n",
    "            if first_letter not in count:\n",
    "                count[first_letter] = {}\n",
    "\n",
    "            if next_letter not in count[first_letter]:\n",
    "                count[first_letter][next_letter] = 0\n",
    "\n",
    "            count[first_letter][next_letter] += 1\n",
    "\n",
    "import json, os\n",
    "\n",
    "if not os.path.exists(\"count.json\"):\n",
    "    os.makedirs(\"count.json\")\n",
    "with open(\"count.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(count, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4781e",
   "metadata": {},
   "source": [
    "## Generating words\n",
    "### Generation function\n",
    "The generation function uses the parameter n for determining the target size of the context it wants to use for prediction. The one letter encoder directly above already sets this value, so we will **not** change it here. Only execute this line if you picked the less efficient encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e467555",
   "metadata": {},
   "source": [
    "Below is the generation function. This generates a single word with some maximum amount of tokens. Each token is a string of predicted letters, so if you used the one letter encoder, each letter is a token. Execute the code below to define the function for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d6cea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_string(start, max_tokens, min_length=1, target_size=n):\n",
    "    string = start\n",
    "    lastPick = string\n",
    "    for i in range(max_tokens):\n",
    "        try:\n",
    "            followUpDict = count[lastPick]\n",
    "        except KeyError:\n",
    "            possibleLastPicks = list(count.keys())\n",
    "            \n",
    "            target = None\n",
    "            difference = 100\n",
    "            for pick in possibleLastPicks:\n",
    "                if string.endswith(pick):\n",
    "                    if abs(target_size - len(pick)) < difference:\n",
    "                        target = pick\n",
    "                        difference = abs(target_size - len(pick))\n",
    "                    \n",
    "            if target is None:\n",
    "                print(f\"{string} does not end in anything expandable.\")\n",
    "                return string\n",
    "\n",
    "            lookUp = target\n",
    "            try:\n",
    "                followUpDict = count[lookUp]\n",
    "            except KeyError:\n",
    "                print(f\"Failed to generate followUp to {lookUp}\")\n",
    "                return string\n",
    "        if len(string) < min_length:\n",
    "            cleanKeys = [key for key in followUpDict.keys() if key != \"\"]\n",
    "            if len(cleanKeys) == 0:\n",
    "                print(f\"{string} is not expandable.\")\n",
    "                return string\n",
    "            cleanValues = [value for key, value in followUpDict.items() if key != \"\"]\n",
    "            lastPick = random.choices(\n",
    "                cleanKeys,\n",
    "                cleanValues,\n",
    "                k=1\n",
    "            )[0]\n",
    "        else:\n",
    "            lastPick = random.choices(list(followUpDict.keys()), list(followUpDict.values()), k=1)[0]\n",
    "        if lastPick == \"\":\n",
    "            print(f\"{string}: legit end picked\")\n",
    "            return string\n",
    "        string += lastPick\n",
    "    print(f\"{string}: Maxed out characters. End might be cut off.\")\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ff516a",
   "metadata": {},
   "source": [
    "Before we can actually use the function, we will need to read the cache our encoder provided. We do this using the code below. Execute it each time you make changes to count.json or run the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8569dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"count.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    count = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d05c0",
   "metadata": {},
   "source": [
    "Now we are almost ready. Let's define the minimum length of our result string in characters, default 4, and the maximum length in tokens, default 9. Run the code below to apply your changes and save the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e12992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = 4\n",
    "maximum = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1fe5e4",
   "metadata": {},
   "source": [
    "### Generating\n",
    "Now we are free to request new words. Just calling the generate_string function with the correct parameters is enough to return a new string. The code below will print out one generated word each time it is executed. Try it out! Debugging information will be provided alongside your actual result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe3d571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dredendun: Maxed out characters. End might be cut off.\n",
      "Dredendun\n"
     ]
    }
   ],
   "source": [
    "print(generate_string(\"\", maximum, minimum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe58d9",
   "metadata": {},
   "source": [
    "If we want to mass-generate words, we can generate a list of words and save it to a file. Change the wordCount to however many words you need and execute the code below to save your new words to a file. Open results.txt in your favorite text editor to see and change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65a41658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tome: Maxed out characters. End might be cut off.\n",
      "Aung: legit end picked\n",
      "Figter: legit end picked\n",
      "MÃ¤man: Maxed out characters. End might be cut off.\n",
      "mÃ¤hr: Maxed out characters. End might be cut off.\n",
      "Arige: Maxed out characters. End might be cut off.\n",
      "Baun: Maxed out characters. End might be cut off.\n",
      "Skerboh: Maxed out characters. End might be cut off.\n",
      "fensgsge: legit end picked\n",
      "unouni: Maxed out characters. End might be cut off.\n",
      "keisen: Maxed out characters. End might be cut off.\n",
      "Pansck: Maxed out characters. End might be cut off.\n",
      "Veng: legit end picked\n",
      "sckunbe: legit end picked\n",
      "Dienb: Maxed out characters. End might be cut off.\n",
      "Jateletem: Maxed out characters. End might be cut off.\n",
      "Preit: Maxed out characters. End might be cut off.\n",
      "Raun: legit end picked\n",
      "Halis: Maxed out characters. End might be cut off.\n",
      "urpe: Maxed out characters. End might be cut off.\n",
      "Enitahri: Maxed out characters. End might be cut off.\n",
      "Spos: legit end picked\n",
      "AuÃuk: Maxed out characters. End might be cut off.\n",
      "KuÃis: Maxed out characters. End might be cut off.\n",
      "Falau: legit end picked\n"
     ]
    }
   ],
   "source": [
    "wordCount = 25\n",
    "\n",
    "words = [generate_string(\"\", random.randint(minimum, maximum), minimum) for _ in range(wordCount)]\n",
    "\n",
    "with open(\"results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
